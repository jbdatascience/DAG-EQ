#+TITLE: Implementation notes


* TODO-List

** DONE Implement data generation
CLOSED: [2019-12-10 Tue 16:10]
- [X] random graph
  - Erdős-Rényi (ER)
  - scale-free (SF)
- [X] random weights
- gaussian noise
- [-] different models
  - [X] linear model
  - [ ] generalized linear
  - [ ] non-linear model
  - additive gaussian noise

** CANCELED Implement some cdt algorithms in Julia
CLOSED: [2019-12-06 Fri 18:08]
- CGNN
- NOTEARS
- DAG-GNN
- GraN-DAG
- meta-transfer

** CANCELED Implement traditional algorithms
CLOSED: [2019-12-06 Fri 18:08]
- [X] PC
- [X] FCI
- LiNGRAM
- GIES
- CAM

** TODO Implement supervised re-parametric
Train:
- assume linear gaussian model
- design a new convolution (on graph), probably use GCN
- generate random model
- generate mu and sigma
- train f(mu, sigma)=adjacent_matrix

Open problems:
- conv operator, shared weights
- order of nodes
- generalized reparametric

Inference:
- compute statistics mu and sigma
- f(mu, sigma)

*** negative sampling
Use a distance measure, or AIC/BIC score to assign the score for all (or a
sample) graphs, not just the correct graph

*** Intergrate with (noise-based?) assymetrcis
For distinguishing equivalent classes

** GAN for MNIST
*** DONE test python code for GAN
CLOSED: [2019-12-10 Tue 16:07]
*** DONE debug DCGAN for julia
CLOSED: [2019-12-10 Tue 16:06]

My experience:
- dropout is very important, use in discriminator
- dropout can be used together with batchnorm
- use bias = false is not required
- the last conv in generator should not be stride=2, but 1, i.e. no scaling
  performed in the last conv
- normalize MNIST to -1,1 instead of 0,1 seems to be very important

*** Python GAN references
- PyTorch-GAN 5k stars https://github.com/eriklindernoren/PyTorch-GAN
- https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
- tensorflow official DCGAN
- pytorch official DCGAN

** TODO Implement interventional loss

*** TODO use dense GAN for graphical model
- generator
- discriminator

*** TODO implement causal effect inference (compute effect)
- [X] hard intervention
- soft intervention
- multiple interventions

*** TODO implement interventional loss
- random intervention
- compute effect
- discriminator likelyhood

*** TODO train with interventional loss

** More Ideas
*** different interventions
- hard do-notation
- hard do-distribution
- soft intervention
- mechanism change

*** Separating interventional distributions
When there are many variables, and many interventions, and the interventional
data might be much less of amount then observational data, it might be
challenging to learn a generative model of the mixture distribution. Thus, we
might consider an extension of this work to seperated unknown interventions.

- we can learn generative models for each of the distribution
- we then use the minimum loss of discriminator to calculate interventional
  loss, i.e. as long as one of the interventional distribution is consistent
  with the random intervention, we accept it.



* old TODO-list

** TODO run those VAEs
** TODO run those GANs
** DONE julia?
   CLOSED: [2019-10-03 Thu 12:14]
Read flux.jl code
** TODO math equation data exp
** TODO interventional loss function exp

** clean up generative models
*** GAN
*** VAE

** unsupervised representation learning
*** InfoGAN
*** NOTEARS
*** Interventional Loss

** Causal generative models
*** TODO GAN
*** TODO VAE
*** TODO InfoGAN
*** TODO NOTEARS
*** TODO Causal


** TODO run causal discovery

*** constraint based methods
- PC
- FCI: can handle confounders
*** score based
- Greedy Equivalence Search (GES)
- FGS

Scores:
- BIC
- AIC

*** inside equivalent class
non-Gaussian or non-Linear

- LiNGAM: Linear Non-Gaussian Acyclic Model:
  https://sites.google.com/site/sshimizu06/lingam
- no-linear model: seems to be extension to LiNGAM, do not have a special
  algorithm, still use noise footprint.


* Data
** synthetic data generation
graph type:
- Erdős-Rényi (ER)
- scale-free (SF)

* Causal Toolbox
** DONE [#A] Causal Discovery Toolbox: Uncover causal relationships in Python
CLOSED: [2019-12-02 Mon 13:26]
Package for causal inference in graphs and in the pairwise settings.
https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox

Most of the discovery algorithms call various of R libraries, pcalg, bnlearn,
CAM. There are also some pairwise algorithms, e.g. NCC, where the original
authors write a NN from scratch, and the code consistency is questionable.

This is a good reference, but I'm not using it either.

It contains clean implementations of many recent NN approaches.

** DONE [#A] CausalInference.jl
CLOSED: [2019-12-02 Mon 17:22]
based on =pcalg=
https://github.com/mschauer/CausalInference.jl

Discovery:
- PC
- FCI

But pretty clean. Would use as starting point.

** DONE Code from papers
CLOSED: [2019-12-05 Thu 22:03]
*** cite:2015-ICML-Lopez-Paz-Towards Towards a Learning Theory of Cause-Effect Inference
Code: https://github.com/lopezpaz/causation_learning_theory

*** DONE cite:2018-Journal-Goudet-Learning Learning functional causal models with generative neural networks
CLOSED: [2019-12-05 Thu 22:03]
https://github.com/GoudetOlivier/CGNN, good quality, heavy in OO, also a copy in CDT

*** [#A] cite:2017-CVPR-Lopez-Paz-Discovering Discovering causal signals in images
  CausalDiscoveryToolbox
  https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox has an
  implementation of NCC.

*** DONE [#B] cite:2018-Preprint-Kalainathan-SAM SAM: Structural Agnostic Model, Causal Discovery and Penalized Adversarial Learning
CLOSED: [2019-12-05 Thu 22:01]
https://github.com/Diviyan-Kalainathan/SAM

*** DONE [#A] cite:2018-NIPS-Zheng-Dags DAGs with NO TEARS: Continuous optimization for structure learning
CLOSED: [2019-12-04 Wed 10:20]
NOTEARS https://github.com/xunzheng/notears

*** DONE (2019) cite:2019-ICML-Yu-Dag DAG-GNN: DAG structure learning with graph neural networks
CLOSED: [2019-12-05 Thu 18:14]
DAG-GNN https://github.com/fishmoon1234/DAG-GNN, good quality

*** DONE cite:2020-ICLRSubmit-Author1237-Gradient Gradient-based neural dag learning
CLOSED: [2019-12-05 Thu 18:14]
GraN-DAG https://github.com/kurowasan/GraN-DAG

*** DONE [#B] cite:2020-ICLRSubmit-Author2311-Meta A meta-transfer objective for learning to disentangle causal mechanisms
CLOSED: [2019-12-05 Thu 15:41]
Code: https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon

Seems to be missing something, e.g. ModelA2B.



** DONE R packages
CLOSED: [2019-12-02 Mon 16:35]

*** DONE [#A] cite:2099-Manual-XXX-pcalg R package pcalg
CLOSED: [2019-12-02 Mon 13:23]
https://cran.r-project.org/web/packages/pcalg/index.html

#+begin_example
svn checkout svn://svn.r-forge.r-project.org/svnroot/pcalg/
#+end_example


#+begin_quote
The main algorithms for causal structure learning are PC (for observational data
without hidden variables), FCI and RFCI (for observational data with hidden
variables), and GIES (for a mix of data from observational studies
(i.e. observational data) and data from experiments involving interventions
(i.e. interventional data) without hidden variables). For causal inference the
IDA algorithm, the Generalized Backdoor Criterion (GBC), the Generalized
Adjustment Criterion (GAC) and some related functions are implemented. Functions
for incorporating background knowledge are provided.
#+end_quote

So
- PC
- FCI
- RFCI
- GIES


**** cite:2014-Manual-Markus-More More Causal Inference with Graphical Models in R Package pcalg
Discovery:
- pc(): constraint based
- fci(): generalization of PC, for allowing latent variables
- skeleton(): a subroutine

More discovery:
- rfci(): much faster than FCI
- ges(): score-based
- gies(): a generalization of GES to interventional data
- simy(): simy is a dynamic programming approach, same interface as gies, which
  means interventional data. The cost is exponential, but computes exact optimum
  of BIC score

Inference:
- ida()
- idaFast()
- backdoor(): check if a causal effect is identifiable or not

This paper seems to be a super set of previous.

- PC and GES assumes no hidden variables
- FCI and RFCI can allow hidden variables
- GIES: assume no hidden variables. (HEBI: Jointly observational and
  interventional data).

***** (UAI 2006) A simple approach for finding the globally optimal Bayesian network structure
simy, a dynamic programming approach
***** cite:2012-Journal-Kalisch-Causal Causal inference using graphical models with the R package pcalg
Previous version.
***** (2012) Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graph
GIES
***** (2009) Estimating High-Dimensional Intervention Effects from Observational Data
IDA method to compute cause effect. Seems to be combining PC and generalized
backdoor criterion.

**** cite:2099-Manual-Kalisch-Overview

Some additional discovery:
- lingram: constraint based, no hidden confounders
- fciPlus: constraint based, allow hidden variables (confounders?)

and randDAG generation

***** (2013 UAI) Learning sparse causal models is not NP-hard
FCI+


*** DONE R package bnlearn
CLOSED: [2019-12-02 Mon 15:18]
http://www.bnlearn.com/, by Marco Scutari, looks like a independent hacker.

There is a mirror: https://github.com/cran/bnlearn

Code quality is less than pcalg.

**** DONE cite:2009-Preprint-Scutari-Learning Learning Bayesian Networks with the bnlearn R Package
CLOSED: [2019-12-02 Mon 15:10]

constraint based algorithms
- Grow-Shrink (gs)
- Incremental Association Markov Blanket (iamb)
- Fast Incremental Association (fast.iamb)
- Interleaved Incremental Association (inter.iamb)
- Max-Min Parents and Children (mmpc)

CI tests for discrete data:
- mutual information
- chi square
- fast mutual information (fmi)
- Akaike Information Criterion (aict)

CI tests for continuous data
- linear correlation
- Fisher's Z
- mutual information (mi-g)

Score-based
- Hill-climbing search (hc)

Scores:
- likelihood and log-likelihood
- Akaike Information Criterion (AIC)
- Bayesian IC (BIC)
- Bayesian Dirichlet equivalent score (BDe)
- K2 score
- equivalent Gaussian posterior density (bge): for continuous data

*** sparsebn
Learning Sparse Bayesian Networks from High-Dimensional Data

Not good, very little information.

- https://cran.r-project.org/web/packages/sparsebn/index.html
- https://github.com/itsrainingdata/sparsebn

*** ICP
ICP:
- https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html
- CRAN mirror: https://github.com/cran/InvariantCausalPrediction

nonlinear ICP
- https://github.com/cran/nonlinearICP
- CRAN mirror: https://github.com/cran/nonlinearICP

*** More R packages
- http://CRAN.R-project.org/package=gRain
- http://CRAN.R-project.org/package=gRbase
- http://CRAN.R-project.org/package=gRc
- http://CRAN.R-project.org/package=deal
- https://github.com/cran/CAM: this is removed from CRAN
- https://github.com/ericstrobl/RCIT: this is just a CI test package

** DONE The Tetrad Project: Graphical Causal Models
    CLOSED: [2019-11-30 Sat 17:13]
- homepage: http://www.phil.cmu.edu/tetrad/
- github: https://github.com/cmu-phil/tetrad
- tutorial: https://rawgit.com/cmu-phil/tetrad/development/tetrad-gui/src/main/resources/resources/javahelp/manual/tetrad_tutorial.html
- manual: http://cmu-phil.github.io/tetrad/manual/

To build javadoc:

#+begin_example
mvn javadoc:javadoc
#+end_example

Reading the Tetrad code. The search code is in
=tetrad/tetrad-lib/src/main/java/edu/cmu/tetrad/search=.  Something to pay
attention:
- [X] the synthetic data generation process. Seems to be in
  =tetrad-lib/.../tetrad/algcomparison/simulation= (a bad choice)
- [X] the GUI shows different algorithm in different categories, e.g.
  - constraint/score-based
  - allow confounders or not
  - local (greedy) search or exact search.
  Find them in the code. This turns out to be annotated, using
  =edu.cmu.tetrad.annotation.Algorithm=, and the annotation happens not in
  =search/=, but in =algcomparison/algorithm=. For example:

#+BEGIN_SRC java
@edu.cmu.tetrad.annotation.Algorithm(
        name = "LiNGAM",
        command = "lingam",
        algoType = AlgType.forbid_latent_common_causes,
        dataType = DataType.Continuous
)
@edu.cmu.tetrad.annotation.Algorithm(
        name = "FCI",
        command = "fci",
        algoType = AlgType.allow_latent_common_causes
)
@Bootstrapping
public class Fci implements Algorithm, TakesInitialGraph, HasKnowledge, TakesIndependenceWrapper {}
@edu.cmu.tetrad.annotation.Algorithm(
        name = "FGES",
        command = "fges",
        algoType = AlgType.forbid_latent_common_causes
)
@Bootstrapping
public class Fges implements Algorithm, TakesInitialGraph, HasKnowledge, UsesScoreWrapper {}
#+END_SRC



- [X] Algorithms:
  - CCD: *Cyclic* Causal Discovery algorithm
  - DCI (Distributed Causal Inference): important because related to dataset mixing
  - FAS: fast adjacency search, used in many variants
  - FCI: Fast Causal Inference
    - GFci, "A Hybrid Causal Search Algorithm for Latent Variable Models," JMLR 2016.
  - GES: greedy search, in =Fges.java=, "Optimal structure identification with greedy search"
  - LiNGAM: Lingam.java, "A linear nongaussian acyclic model for causal discovery"
  - PC ("Peter/Clark") algorithm
    - PC Local algorithm

- [X] independence test
  - IndTestChiSquare.java
  - IndTestDSep.java
  - IndTestFisherZ.java

- other
  - MeekRules.java: meek rule seems to relate to background knowledge, "Causal
    inference and causal explanation with background knowledge".

- [X] scores
  - BDe score
  - BIC score
  - Dirichlet Score (seems to be the BDeu score)
  - MVPScore.java, mixed variable polynomial BIC score for fGES?

- [ ] I'll probably also need to implement parameter learning
- [ ] To verify correctness of my implementation, compare the results (e.g. strcture learned, p value)


*** wrappers
These two are really just wrappers. Both provides example data.
- R: https://github.com/bd2kccd/r-causal
- python: https://github.com/bd2kccd/py-causal, this provides many jupyter notebooks

Not very interesting wrappers:
- cmd: https://github.com/bd2kccd/causal-cmd
- web: https://github.com/bd2kccd/causal-web
- REST: https://github.com/bd2kccd/causal-rest-api


** DONE Tübingen group
CLOSED: [2019-12-02 Mon 16:35]
- many papers and source code: http://webdav.tuebingen.mpg.de/causality/
  - they also built the "Database with cause-effect pairs"

Most papers are matlab, Janzing's paper is R.


*** (2008) Nonlinear causal discovery with additive noise models
- code: http://webdav.tuebingen.mpg.de/causality/additive-noise.tar.gz

*** cite:2009-ICML-Peters-Detecting Detecting the Direction of Causal Time Series
- code: http://webdav.tuebingen.mpg.de/causality/online_time_dir_hsic.zip

*** (2009) Distinguishing cause from effect with constrained nonlinear ICA
- code: http://webdav.tuebingen.mpg.de/causality/CauseOrEffect_NICA.rar
  - pure matlab
*** (2010) Causal Markov  condition for submodular information measures
- code: http://personal-homepages.mis.mpg.de/steudel/lzInformation.zip

*** cite:2010-UAI-Zhang-Invariant Invariant Gaussian Process Latent Variable Models and Application in Causal Discovery
*** cite:2010-AISTATS-Peters-Identifying Identifying Cause and Effect on Discrete Data using Additive Noise Models
- code: http://webdav.tuebingen.mpg.de/causality/online_aistats_arxive_discrete.zip

*** cite:2010-ICML-Janzing-Telling Telling cause from effect based on high-dimensional observations
- code: http://webdav.tuebingen.mpg.de/causality/online_trace_method_July_2010.tar.gz

*** cite:2010-UAI-Daniusis-Inferring Inferring deterministic causal relations
- code: http://webdav.tuebingen.mpg.de/causality/igci.tar.gz
  - Joris Mooij, single .m file

*** cite:2010-NIPS-Stegle-Probabilistic Probabilistic latent variable models for distinguishing between cause and effect
- code: http://webdav.tuebingen.mpg.de/causality/nips2010-gpi-code.tar.gz

*** cite:2011-UAI-Peters-Identifiability Identifiability of Causal Graphs using Functional Models
- code: http://webdav.tuebingen.mpg.de/causality/identifiability_dags_snapshot.zip

*** (UAI 2011) Testing whether linear relations are causal: A free probability approach
- code: http://webdav.tuebingen.mpg.de/causality/code_zscheischler.zip

*** cite:2011-UAI-Zhang-Kernel Kernel-based conditional independence test and application in causal discovery
- code: http://people.tuebingen.mpg.de/kzhang/KCI-test.zip

*** [#B] cite:2011-NIPS-Mooij-Causal On Causal Discovery with Cyclic Additive Noise Models
- code: http://webdav.tuebingen.mpg.de/causality/NIPS2011-code.tar.gz
  - C++ and matlab for plotting, pretty good

*** cite:2011-UAI-Janzing-Detecting Detecting low-complexity unobserved causes
- code: http://webdav.tuebingen.mpg.de/causality/UAI2011_Janzing_purity_code.zip

*** (2013) Quantifying causal influences
- code: http://webdav.tuebingen.mpg.de/causality/AOS2013-code.zip

*** cite:2013-UAI-Sgouritsa-Identifying Identifying Finite Mixtures of Nonparametric Product Distributions and Causal Inference of Confounders

*** [#B] (2017) Detecting confounding in multivariate linear models via spectral analysis
- code: http://webdav.tuebingen.mpg.de/causality/confounder_detection_linear.zip
  - R code, readable, with 4 real data

*** [#B] cite:2018-ICML-Janzing-Detecting Detecting non-causal artifacts in multivariate linear regression models
- code: http://webdav.tuebingen.mpg.de/causality/confounder_detection_independent_sources.zip
  - R code, readable, with 3 real data


** Not causal learning
*** TODO MIT
http://probcomp.csail.mit.edu/
**** gen
**** crosscat
https://github.com/probcomp/crosscat

*** tutorial & references

- a blog post http://fastml.com/bayesian-machine-learning/

**** DONE Probabilistic Programming and Bayesian Methods for Hackers
   CLOSED: [2019-11-25 Mon 22:28]
a book, using PyMC3, about probablistic programming in general
http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/

*** Causal inference only
**** DoWhy | Making causal inference easy
https://github.com/microsoft/dowhy
*** Bayesian Network learning
**** Python Library for Probabilistic Graphical Models
https://github.com/pgmpy/pgmpy

**** Bayesian Network Modeling and Analysis
https://github.com/paulgovan/BayesianNetwork


**** Bayesian network Learning Improved Project (blip)
A bayesian network learning code: https://github.com/mauro-idsia/blip
*** Bayesian learning in general
**** PyMC: Probabilistic Programming in Python
https://docs.pymc.io/


** Other

*** pyro: Deep Universal Probabilistic Programming
http://pyro.ai/


*** edwardlib
A library for probabilistic modeling, inference, and criticism.
http://edwardlib.org/


*** ZhuSuan: A Library for Bayesian Deep Learning
https://github.com/thu-ml/zhusuan

*** Stan: Sampling Through Adaptive Neighborhoods
 https://mc-stan.org/

**** The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo
 Stan uses Nuts as sampler.

 #+begin_quote
 Most of the computation [in Stan] is done using Hamiltonian Monte Carlo. HMC
 requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the
 “No-U-Turn Sampler”) which optimizes HMC adaptively. In many settings, Nuts is
 actually more computationally efficient than the optimal static HMC!
 #+end_quote

**** Automatic Variational Inference in Stan
 https://arxiv.org/abs/1506.03431

 #+begin_quote
 Variational inference is a scalable technique for approximate Bayesian
 inference. Deriving variational inference algorithms requires tedious
 model-specific calculations; this makes it difficult to automate. We propose an
 automatic variational inference algorithm, automatic differentiation variational
 inference (ADVI). The user only provides a Bayesian model and a dataset; nothing
 else.
 #+end_quote

*** Infer.NET by Microsoft
https://dotnet.github.io/infer/
