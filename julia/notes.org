#+TITLE: Implementation notes


* TODO-List

** DONE Implement data generation
CLOSED: [2019-12-10 Tue 16:10]
- [X] random graph
  - Erdős-Rényi (ER)
  - scale-free (SF)
- [X] random weights
- gaussian noise
- [-] different models
  - [X] linear model
  - [ ] generalized linear
  - [ ] non-linear model
  - additive gaussian noise

** CANCELED Implement some cdt algorithms in Julia
CLOSED: [2019-12-06 Fri 18:08]
- CGNN
- NOTEARS
- DAG-GNN
- GraN-DAG
- meta-transfer

** CANCELED Implement traditional algorithms
CLOSED: [2019-12-06 Fri 18:08]
- [X] PC
- [X] FCI
- LiNGRAM
- GIES
- CAM

** GAN for MNIST
*** DONE test python code for GAN
CLOSED: [2019-12-10 Tue 16:07]
*** DONE debug DCGAN for julia
CLOSED: [2019-12-10 Tue 16:06]

My experience:
- dropout is very important, use in discriminator
- dropout can be used together with batchnorm
- use bias = false is not required
- the last conv in generator should not be stride=2, but 1, i.e. no scaling
  performed in the last conv
- normalize MNIST to -1,1 instead of 0,1 seems to be very important

*** Python GAN references
- PyTorch-GAN 5k stars https://github.com/eriklindernoren/PyTorch-GAN
- https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
- tensorflow official DCGAN
- pytorch official DCGAN

** TODO Implement interventional loss

*** TODO use dense GAN for graphical model
- generator
- discriminator

*** TODO implement causal effect inference (compute effect)
- [X] hard intervention
- soft intervention
- multiple interventions

*** TODO implement interventional loss
- random intervention
- compute effect
- discriminator likelyhood

*** STARTED related work
Looks like I have to compare against them, so it does not hurt implementing them now.

**** TODO The NOTEARS framework
NOTEARS does not seem to recover beyond equivalent class.

I'm at the optimization solver, and currently
- Optim uses autodiff. However, it is slow, and does not seem to solve correctly
- I'm trying something else, like NLopt suite, and this seems to work

***** TODO score metrics
And the score seems to be very different when I modify notears's python code with:
- replace 2 * d * d with just the result and gradient of d*d
- the L1 regularizer also matters

**** meta-transfer
And the +NOTEARS version

**** traditional
- PC
- LiNGRAM
- GES (using FGS)


*** TODO train with interventional loss

** More Ideas on Interventional loss
*** different interventions
- hard do-notation
- hard do-distribution
- soft intervention
- mechanism change

*** Separating interventional distributions
When there are many variables, and many interventions, and the interventional
data might be much less of amount then observational data, it might be
challenging to learn a generative model of the mixture distribution. Thus, we
might consider an extension of this work to seperated unknown interventions.

- we can learn generative models for each of the distribution
- we then use the minimum loss of discriminator to calculate interventional
  loss, i.e. as long as one of the interventional distribution is consistent
  with the random intervention, we accept it.

** TODO supervised re-parametric
Train:
- assume linear gaussian model
- design a new convolution (on graph), probably use GCN
- generate random model
- generate mu and sigma
- train f(mu, sigma)=adjacent_matrix

Open problems:
- conv operator, shared weights
- order of nodes
- generalized reparametric

Inference:
- compute statistics mu and sigma
- f(mu, sigma)

*** negative sampling
Use a distance measure, or AIC/BIC score to assign the score for all (or a
sample) graphs, not just the correct graph

*** Intergrate with (noise-based?) assymetrcis
For distinguishing equivalent classes


* old TODO-list

** TODO run those VAEs
** TODO run those GANs
** DONE julia?
   CLOSED: [2019-10-03 Thu 12:14]
Read flux.jl code
** TODO math equation data exp
** TODO interventional loss function exp

** clean up generative models
*** GAN
*** VAE

** unsupervised representation learning
*** InfoGAN
*** NOTEARS
*** Interventional Loss

** Causal generative models
*** TODO GAN
*** TODO VAE
*** TODO InfoGAN
*** TODO NOTEARS
*** TODO Causal


** TODO run causal discovery

*** constraint based methods
- PC
- FCI: can handle confounders
*** score based
- Greedy Equivalence Search (GES)
- FGS

Scores:
- BIC
- AIC

*** inside equivalent class
non-Gaussian or non-Linear

- LiNGAM: Linear Non-Gaussian Acyclic Model:
  https://sites.google.com/site/sshimizu06/lingam
- no-linear model: seems to be extension to LiNGAM, do not have a special
  algorithm, still use noise footprint.

