#+TITLE: Implementation notes

* References
** Code from papers
- https://github.com/GoudetOlivier/CGNN, good quality, heavy in OO, also a copy in CDT
- CausalDiscoveryToolbox
  https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox has an
  implementation of NCC.
- https://github.com/Diviyan-Kalainathan/SAM
- NOTEARS https://github.com/xunzheng/notears
- DAG-GNN https://github.com/fishmoon1234/DAG-GNN, good quality
- GraN-DAG https://github.com/kurowasan/GraN-DAG
- meta-transfer: https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon

* Supervised Results

Several points:
1. training time: increase linear with model depth, but increase less than
   linear for graph size =d=, almost stay the same. This is because the hidden
   layer does not scale with =d=, remains the same
2. FC model with dropout is hard to fit
3. FC model is not scalable to large graphs
4. FC model generally converge at 150k

For EQ models:
1. training time
   - increases linear with model depth
   - increases more than linear with graph size =d=. That is because of two
     parts: the input/output size, as well as in each layer: the hidden layer
     computation cost is linear to =d=
2. It is much easier to fit
3. EQ models generally converge at 30k
4. deep EQ models performs very well

| model   |  d | loss | prec | recall | nsteps | time        | step/s | time / 10k | comment      |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| FC      |  5 |      | 0.81 |        | 150k   | 2m50s       |        |            |              |
|         |  7 |      | 0.71 |        | 150k   | 3m          |        |            |              |
|         | 10 |      | 0.67 |        | 150k   | 3m          |        |            |              |
|         | 15 |      | 0.56 |        | 200k   | 4m40s       |        |            |              |
|         | 20 |      | 0.45 |        | 300k   | 8m11s       |        |            | not converge |
|         | 25 |      |    0 |        | 300k   | 9m31s       |        |            |              |
|         | 30 |      |    0 |        | 300k   | 11m33s      |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| deep FC |  5 |      | 0.92 |        | 150k   | 8m40s       |        |            |              |
|         |  7 |      | 0.83 |        | 150k   | 8m50s       |        |            |              |
|         | 10 |      | 0.64 |        | 150k   | 9m1s        |        |            |              |
|         | 15 |      |    0 |        | 300k   | 18m22s      |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| EQ      |  5 |      |      |   0.79 | 30k    | 16m         |        |            |              |
|         |  7 |      |      |   0.75 | 30k    | 26m         |        |            |              |
|         | 10 |      |      |   0.69 | 30k    | 38m         |        |            |              |
|         | 15 |      |      |        | 30k    | (est 1h20m) |        |            |              |
|         | 20 |      |      |        | 30k    | (est 2h)    |        |            |              |
|         | 25 |      |      |        |        |             |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| deep-EQ |  5 |      | 0.89 |        | 30k    | 43m         |        |            |              |
|         | 10 |      | 0.81 |        | 30k    | 1h46m       |        |            |              |
|         | 15 |      | 0.76 |        | 30k    | 3h40m       |        |            |              |
|         | 20 |      | 0.76 |        | 30k    | 6h51m       |        |            |              |
|         | 25 |      |      |        |        |             |        |            |              |
|         | 30 |      |      |        |        |             |        |            |              |


* Supervised full DAG TODO

** universal EQ model

Note that EQ model parameters does not depend on d. Each layer has exactly 5
weights, no matter the size of graph.

Thus it may be possible to train a universal EQ model that works on different
graph. I can approach this in several ways:
1. directly transfer
   2. fine tune some layer?
3. train on different size graphs

Looks like the direct transfer works.

*** STARTED train on different size graphs

** STARTED continual training

1. choose a larger save steps, e.g. 1k
2. save the model as modelID-1000.bson
3. when continual training, check if model loadable. Choose the most recent, and
   set current steps accordingly

NOTE: the tensorboard logs must be kept consistent, and only do append

The only downside would be the time will be inaccurate. Maybe I should (HEBI:
record time as well in filename) (HEBI: record time as well in filename ...),
together with steps. After all, these are the only two thing I need.



*** STARTED continual training debug
*** DONE cudatasetiterator convert debug
CLOSED: [2020-03-12 四 11:04]


** TODO debug EQ performance
Not matching previous. Maybe previous result is binary C=1? Try it, and write
exp option for this.

Actually the deep model seems to preserve the performance.

** TODO Compare
*** TODO pairwise supervised model
*** TODO unsupervised model
  - NOTEARS
  - PC

** TODO Real data
- pairwise?
- non-pairwise but sparse?
- different graph type in synthetic data

** Synthetic data
- table for different models and settings
  - network settings:
    - dropout batchnorm vs. none
    - network depth
    - network width
    - FC vs. EQ
    - batch size
    - learning rate
  - metrics
    - accuracy and recall
    - # iterations
    - time
    - # parameters
- plot AUC for different threshold
- plot training process



* Do-loss TODO

** New round
*** CANCELED use mixture multi-variant Gaussian to fit the data
CLOSED: [2020-01-09 Thu 18:28]
I should not use gaussian mixture, as that's not flexible, I need to know how
many components.

- observational
- 1 interventional
- 2+ interventional
- observational + 1 interventional
- observational + 2+ interventional

*** TODO use mixture Gaussian likelihood as oracle
to fit the do-loss

- I probably try to derive the closed form first

*** TODO sample interventions?
*** TODO sample from cyclic intermediate graph state


** TODO Implement interventional loss
*** TODO use dense GAN for graphical model
- generator
- discriminator

*** TODO implement causal effect inference (compute effect)
- [X] hard intervention
- soft intervention
- multiple interventions

*** TODO implement interventional loss
- random intervention
- compute effect
- discriminator likelyhood

*** Tuebingen pairs
- NOTEARS does not work
- implement LiNGRAM to see if it works
- But it does not seem to have interventional data, and does not have ground
  truth SEM to generate interventional data
- what are the interventional data out there?
- what is the optimization the meta-transfer is using?

*** related work
Looks like I have to compare against them, so it does not hurt implementing them now.

**** DONE The NOTEARS framework
CLOSED: [2019-12-11 Wed 17:00]
NOTEARS does not seem to recover beyond equivalent class.

I'm at the optimization solver, and currently
- Optim uses autodiff. However, it is slow, and does not seem to solve correctly
- I'm trying something else, like NLopt suite, and this seems to work

https://github.com/xunzheng/notears

***** DONE score metrics
CLOSED: [2019-12-11 Wed 18:09]
And the score seems to be very different when I modify notears's python code with:
- replace 2 * d * d with just the result and gradient of d*d
- the L1 regularizer also matters

***** DONE non-negative box constraint
CLOSED: [2019-12-11 Wed 18:09]
https://github.com/xunzheng/notears/issues/5

and related: 2*d*d w_est problem
***** TODO why NOTEARS can distinguish A->B and B->A?

**** DONE meta-transfer in julia
CLOSED: [2019-12-20 Fri 12:56]
And the +NOTEARS version

Several problems:
1. Zygote cannot differentiate through likelihood:
   https://github.com/FluxML/Zygote.jl/issues/436
2. Tracker.jl TrackedArrays cannot work through logsumexp's mapreduce

Thus it is basically impossible to get it work. But it's good enough, I
understand how it works, let's implement do-loss.

**** traditional
- PC
- LiNGRAM
- GES (using FGS)


*** TODO train with interventional loss

** More Ideas on Interventional loss
*** different interventions
- hard do-notation
- hard do-distribution
- soft intervention
- mechanism change

*** Separating interventional distributions
When there are many variables, and many interventions, and the interventional
data might be much less of amount then observational data, it might be
challenging to learn a generative model of the mixture distribution. Thus, we
might consider an extension of this work to seperated unknown interventions.

- we can learn generative models for each of the distribution
- we then use the minimum loss of discriminator to calculate interventional
  loss, i.e. as long as one of the interventional distribution is consistent
  with the random intervention, we accept it.


* old TODO-list

** TODO run those VAEs
** TODO run those GANs
** DONE julia?
   CLOSED: [2019-10-03 Thu 12:14]
Read flux.jl code
** TODO math equation data exp
** TODO interventional loss function exp

** clean up generative models
*** GAN
*** VAE

** unsupervised representation learning
*** InfoGAN
*** NOTEARS
*** Interventional Loss

** Causal generative models
*** TODO GAN
*** TODO VAE
*** TODO InfoGAN
*** TODO NOTEARS
*** TODO Causal

** Number of unique DAG

https://oeis.org/A003024

|  d | #dag |
|----+------|
|  1 |      |
|  2 |      |
|  3 |      |
|  4 |      |
|  5 |      |
|  6 |      |
|  7 |      |
|  8 |      |
|  9 |      |
| 10 |      |
| 11 |      |


** TODO run causal discovery

*** constraint based methods
- PC
- FCI: can handle confounders
*** score based
- Greedy Equivalence Search (GES)
- FGS

Scores:
- BIC
- AIC

*** inside equivalent class
non-Gaussian or non-Linear

- LiNGAM: Linear Non-Gaussian Acyclic Model:
  https://sites.google.com/site/sshimizu06/lingam
- no-linear model: seems to be extension to LiNGAM, do not have a special
  algorithm, still use noise footprint.


* DONE-list
** DONE Implement data generation
CLOSED: [2019-12-10 Tue 16:10]
- [X] random graph
  - Erdős-Rényi (ER)
  - scale-free (SF)
- [X] random weights
- gaussian noise
- [-] different models
  - [X] linear model
  - [ ] generalized linear
  - [ ] non-linear model
  - additive gaussian noise

** CANCELED Implement some cdt algorithms in Julia
CLOSED: [2019-12-06 Fri 18:08]
- CGNN
- NOTEARS
- DAG-GNN
- GraN-DAG
- meta-transfer

** CANCELED Implement traditional algorithms
CLOSED: [2019-12-06 Fri 18:08]
- [X] PC
- [X] FCI
- LiNGRAM
- GIES
- CAM

** DONE GAN for MNIST
CLOSED: [2019-12-18 Wed 12:42]
*** DONE test python code for GAN
CLOSED: [2019-12-10 Tue 16:07]
*** DONE debug DCGAN for julia
CLOSED: [2019-12-10 Tue 16:06]

My experience:
- dropout is very important, use in discriminator
- dropout can be used together with batchnorm
- use bias = false is not required
- the last conv in generator should not be stride=2, but 1, i.e. no scaling
  performed in the last conv
- normalize MNIST to -1,1 instead of 0,1 seems to be very important

*** Python GAN references
- PyTorch-GAN 5k stars https://github.com/eriklindernoren/PyTorch-GAN
- https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
- tensorflow official DCGAN
- pytorch official DCGAN

** CANCELED supervised re-parametric
CLOSED: [2019-12-18 Wed 12:41]
Train:
- assume linear gaussian model
- design a new convolution (on graph), probably use GCN
- generate random model
- generate mu and sigma
- train f(mu, sigma)=adjacent_matrix

Open problems:
- conv operator, shared weights
- order of nodes
- generalized reparametric

Inference:
- compute statistics mu and sigma
- f(mu, sigma)
*** TODO ensure acyclic in evaluation

*** DONE test on different graph
CLOSED: [2019-12-16 Mon 14:31]
for 5-var case, to see what's the scalability problem

It does not work, so the problem is not the size of graph. With different graphs
as test data, it does not work.

But there is some good news:
1. NN fits training data without any problem
2. with more N per graph, it improves

*** TODO reuse 5-var learned model on 20 var

*** TODO Regularizing
- it can reach high accuracy very quickly
- but it seems to overfitting very quickly as well

*** CANCELED negative sampling
CLOSED: [2019-12-12 Thu 16:34]
Use a distance measure, or AIC/BIC score to assign the score for all (or a
sample) graphs, not just the correct graph

*** TODO multi-variate
*** TODO stochastic SGD
- I need more data
- when data is large, I need to batch
- and I need to shuffle the batches as well, for stochastic
*** TODO other models
- different number of hidden units
- different number of layers
- different models, e.g. GCN

*** TODO sharing weights
For generalizing to other number of variables
*** TODO generalizing to other models
- linear + Gaussian
- non-Gaussian, e.g. poisson
- generalized linear
- non-linear


*** Intergrate with (noise-based?) assymetrcis
- For distinguishing equivalent classes
  - but it alrady works very well
- using data other than statistics


* Supervised full DAG DONE

** China
*** DONE Look for other potential bugs in equivariant model
CLOSED: [2020-03-01 日 14:03]
Mostly the correcteness of gradient calculation and broadcasting.

*** DONE use cross entropy
CLOSED: [2020-02-29 六 17:36]
*** DONE figure out how to make it run faster
CLOSED: [2020-02-29 六 17:36]
*** CANCELED precision is low, but recall is high
CLOSED: [2020-03-01 日 14:00]
** CANCELED supervised learning with equivariant model
CLOSED: [2020-01-12 Sun 12:40]

Does not work.

*** large n
*** verify gradient computation
*** add bias
*** use max-pool
*** normalization & regularization

*** DONE gpu
CLOSED: [2020-01-12 Sun 11:55]
*** DONE more data points
CLOSED: [2020-01-12 Sun 11:55]
Does not help.

*** DONE init function
CLOSED: [2020-01-12 Sun 12:40]
simply 0

This does not work, the model parameters does not change at all.

*** DONE compare parameters
CLOSED: [2020-01-12 Sun 12:39]

*** DONE try previous MLP
CLOSED: [2020-01-10 Fri 13:01]

Delta debuggging what is wrong

It is the 1 dim (5,5,1,100)


** DONE exp model setting
CLOSED: [2020-03-02 一 17:34]

- dropout, batchnorm or None
  - looks like dropout is not working
- nlayer
- depth
- width
- activation
- learning rate

** DONE performance of FC models
CLOSED: [2020-03-02 一 17:34]
Why it is slow bad now? Probably:
- data generating with [0.5,2]
- threshold
- sigmoid activation
- MSE loss


** DONE data
CLOSED: [2020-03-02 一 17:34]

Data:
- unit data (W=1)
- [-2,-0.5] data
- non-univariance data
- non-linear model

** DONE loss
CLOSED: [2020-03-02 一 17:34]
Loss:
- cross entropy vs. MSE

use MSE, because they seems to perform similarly, and xent only supports binary
classification.

** CANCELED learning rate (decay)

** TODO persistence
*** DONE integrate with reading tensorboard logs
CLOSED: [2020-03-04 三 21:35]

*** DONE saved model
CLOSED: [2020-03-04 三 21:34]
- save at multiple points: not very urgent

For synthetic data, not very useful, because tensorboard already logs the loss
and accuracy metrics.

For real data this is necessary.

*** CANCELED continue training
CLOSED: [2020-03-02 一 19:36]
with unique ID support

This might not make sense, because
1. I need to implement logic of resuming
2. I need to implement logic for restoring "step" count
3. the time metrics would be wrong

A lot of overhead.

*** CANCELED seed and averaging
CLOSED: [2020-03-02 一 19:36]

Not very urgent.
*** CANCELED profiling
CLOSED: [2020-03-02 一 17:50]
If profiling is not costly, I can probably record for each run

** DONE verify performance
CLOSED: [2020-03-06 五 12:24]
Wait for all runs and see:
- debug dropout performance
- debug FC vs. EQ and -deep version performance

