#+TITLE: Implementation notes

* References
** Code from papers
- https://github.com/GoudetOlivier/CGNN, good quality, heavy in OO, also a copy in CDT
- CausalDiscoveryToolbox
  https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox has an
  implementation of NCC.
- https://github.com/Diviyan-Kalainathan/SAM
- NOTEARS https://github.com/xunzheng/notears
- DAG-GNN https://github.com/fishmoon1234/DAG-GNN, good quality
- GraN-DAG https://github.com/kurowasan/GraN-DAG
- meta-transfer: https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon

* TODO NeurIPS 2020
** Misc
*** TODO And add baseline of random guessing
*** data efficiency
*** testing for variations
- [X] ER2, ER4
- [ ] Exp, Gumbel
- [X] MLP
  - FIXME OK, I probably need to try some other non-linear model, or just
    non-linear does not work

** Important

- [-] compare with pairwise-supervised approach
  - [X] and the code up the traditional approach experiment code
- [-] 10, 20, 50, 100 should be ideal graph size
- [ ] close comparison between flat-CNN and deep-EQ
  - maybe try CNN with larger filters?
- [X] add MLP testing results
  - [ ] I could probably ignore MLP experiment for now

*** TODO ensemble SF and ER
Looks like our method performs so much better on SF than ER. I either:
- figure out if there's anything wrong, or
- ensemble training on SF and ER

*** TODO record featurization time

** Real data
*** cite:2005-Science-Sachs-Causal Causal protein-signaling networks derived from multiparameter single-cell data
Sachs 2005 dataset

- http://www.sciencemag.org/content/suppl/2005/04/21/308.5721.523.DC1/Sachs.SOM.Datasets.zip
  - (HEBI: why there are so many csv files?)
- looks like there's also interventional data:
  https://github.com/snarles/causal/blob/master/bnlearn_files/sachs.discretised.txt

*** bnlearn data
- https://www.bnlearn.com/bnrepository/gaussian-medium.html#ecoli70
- https://www.bnlearn.com/bnrepository/#alarm

Most of these are discrete. My approach should work on discrete too, maybe a different featurization.

*** tuebingen pairs

- https://www.ccd.pitt.edu/wiki/index.php/Data_Repository
- http://webdav.tuebingen.mpg.de/cause-effect/


** STARTED implement approaches to compare

- [X] PC, from pcalg package, also in cdt
- [X] FGS cite:2017-Journal-Ramsey-Million https://github.com/bd2kccd/py-causal
  - GES cite:2002-JMLR-Chickering-Learning, from pcalg, also in cdt
- [-] LiNGRAM cite:2006-JMLR-Shimizu-Linear, from pcalg, also in cdt
  - author's page https://sites.google.com/site/sshimizu06/lingam
- [-] CAM cite:2014-Journal-Buhlmann-CAM, from cdt, wrapper for an R script
  - cdt: https://github.com/FenTechSolutions/CausalDiscoveryToolbox
  - was on CRAN https://cran.r-project.org/web/packages/CAM/index.html

Notears has very clean API, there's no problem of plugging in my data.
- [-] Notears cite:2018-NIPS-Zheng-Dags cite:2018-cs.LG-Zheng-Dags https://github.com/xunzheng/notears

| model | EQ | CNN baseline | NOTEARS | FGS |
|-------+----+--------------+---------+-----|
| d=50  | 23 |           42 |      20 |  10 |
| d=100 | 62 |           93 |      50 |  80 |


For these three, the model, trainig, and data loading are highly tangled
together. I thus need to figure out, for each of them, the data loading
scheme. Then, I'll have to do two parts:
1. transform my data into their format
2. possibly need to write loader and data wrapper in their code
Then I'll just run their code and get whatever they are reporting.

- [ ] DAG-GNN cite:2019-ICML-Yu-Dag https://github.com/fishmoon1234/DAG-GNN
  - ths is non-linear extension. But this seems to be the easiest to modify
- [ ] RL-BIC cite:2020-ICLROral-Zhu-Causal https://github.com/huawei-noah/trustworthyAI
  - (HEBI: this is super slow) for even a single data point, about 100 iter/min,
    where there seems to be a lot of iterations

- [-] GraN-DAG cite:2020-ICLRPoster-Lachapelle-Gradient https://github.com/kurowasan/GraN-DAG
  - (HEBI: the code is too tangled to run. Running time is long)
  - this is non-linear only, so pass
- [-] pairwise supervised model cite:2015-ICML-Lopez-Paz-Towards
- [-] Structural Agnostic Model (SAM)


| model    | where     |
|----------+-----------|
| PC       | cdt       |
| FGS      | py-causal |
| LiNGRAM  | cdt       |
| CAM      | cdt       |
|----------+-----------|
| Notears  | github    |
| DAG-GNN  | github    |
| GraN-DAG | github    |
| RL-BIC   | github    |

*** DONE [#A] cite:2099-Manual-XXX-pcalg R package pcalg
CLOSED: [2019-12-02 Mon 13:23]
https://cran.r-project.org/web/packages/pcalg/index.html

#+begin_example
svn checkout svn://svn.r-forge.r-project.org/svnroot/pcalg/
#+end_example
So
- PC
- FCI
- RFCI
- GIES

**** cite:2014-Manual-Markus-More More Causal Inference with Graphical Models in R Package pcalg
Discovery:
- pc(): constraint based
- fci(): generalization of PC, for allowing latent variables
- skeleton(): a subroutine

More discovery:
- rfci(): much faster than FCI
- ges(): score-based
- gies(): a generalization of GES to interventional data
- simy(): simy is a dynamic programming approach, same interface as gies, which
  means interventional data. The cost is exponential, but computes exact optimum
  of BIC score

- PC and GES assumes no hidden variables
- FCI and RFCI can allow hidden variables
- GIES: assume no hidden variables. (HEBI: Jointly observational and
  interventional data).

*** DONE [#A] Causal Discovery Toolbox: Uncover causal relationships in Python
CLOSED: [2019-12-02 Mon 13:26]
Package for causal inference in graphs and in the pairwise settings.
https://github.com/FenTechSolutions/CausalDiscoveryToolbox

Most of the discovery algorithms call various of R libraries, pcalg, bnlearn,
CAM. There are also some pairwise algorithms, e.g. NCC, where the original
authors write a NN from scratch, and the code consistency is questionable.

This is a good reference, but I'm not using it either.

It contains clean implementations of many recent NN approaches.
*** DONE [#A] CausalInference.jl
CLOSED: [2019-12-02 Mon 17:22]
based on =pcalg=
https://github.com/mschauer/CausalInference.jl

Discovery:
- PC
- FCI

But pretty clean. Would use as starting point.


* [Paper] improve existing experiment

** real data experiment
More real data

** TODO different types of graphs: ER vs. SF vs. some others


** TODO different weight matrix range

weights range [0.5, 1.5*k] for k=1/3, 1/2, 1, 2, 3, ..., ?


** TODO different noise model

(and other noise models in Notears paper if you don't want to restrict to
Gaussian noise model).

* [Paper] missing components

** TODO compare with pairwise supervised learning
\cite{2019-JMLR-Hill-Causal} and \cite{2015-ICML-Lopez-Paz-Towards}

** TODO Compare with graphical and optimization approaches

- constraint-based methods, PC~\cite{2000-Book-Spirtes-Causation} and
  FCI~\cite{2000-Book-Spirtes-Causation}
- score-based methods, GES~\cite{2002-JMLR-Chickering-Learning} (the fast
  implementation, FGS~\cite{2017-Journal-Ramsey-Million}), and
- optimization based method, NOTEARS~\cite{2018-NIPS-Zheng-Dags}

** TODO comparison with traditional constraint and score based methods, and latest methods

Need to compare with FGS, Notears, and latest algorithms: DAG-GNN in icml-19,
RL-BIC (CAUSAL DISCOVERY WITH REINFORCEMENT LEARNING, ICLR 2020), GraN-DAG
(GRADIENT-BASED NEURAL DAG LEARNING, ICLR 2020). Among them, RL-BIC is limited
to d<=30. All others work for larger d. It'd be important if this algorithm can
work for larger d. If you need computation power, do you think LAS ResearchIT
GPU clusters "pronto" could be useful for you?

** advantage
The major advantage of ours is:
- data driven full-DAG discovery
- fast inference. Although training effort is significant compared to other
  causal discovery methods, our model has significantly lower inference time,
  and can be easily batched for parallel discovery.

** DONE real data experiment
CLOSED: [2020-05-15 Fri 15:54]

** DONE cross graph type evaluation?
CLOSED: [2020-05-15 Fri 15:54]



* Supervised Results

Several points:
1. training time: increase linear with model depth, but increase less than
   linear for graph size =d=, almost stay the same. This is because the hidden
   layer does not scale with =d=, remains the same
2. FC model with dropout is hard to fit
3. FC model is not scalable to large graphs
4. FC model generally converge at 150k

For EQ models:
1. training time
   - increases linear with model depth
   - increases more than linear with graph size =d=. That is because of two
     parts: the input/output size, as well as in each layer: the hidden layer
     computation cost is linear to =d=
2. It is much easier to fit
3. EQ models generally converge at 30k
4. deep EQ models performs very well

| model   |  d | loss | prec | recall | nsteps | time        | step/s | time / 10k | comment      |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| FC      |  5 |      | 0.81 |        | 150k   | 2m50s       |        |            |              |
|         |  7 |      | 0.71 |        | 150k   | 3m          |        |            |              |
|         | 10 |      | 0.67 |        | 150k   | 3m          |        |            |              |
|         | 15 |      | 0.56 |        | 200k   | 4m40s       |        |            |              |
|         | 20 |      | 0.45 |        | 300k   | 8m11s       |        |            | not converge |
|         | 25 |      |    0 |        | 300k   | 9m31s       |        |            |              |
|         | 30 |      |    0 |        | 300k   | 11m33s      |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| deep FC |  5 |      | 0.92 |        | 150k   | 8m40s       |        |            |              |
|         |  7 |      | 0.83 |        | 150k   | 8m50s       |        |            |              |
|         | 10 |      | 0.64 |        | 150k   | 9m1s        |        |            |              |
|         | 15 |      |    0 |        | 300k   | 18m22s      |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| EQ      |  5 |      |      |   0.79 | 30k    | 16m         |        |            |              |
|         |  7 |      |      |   0.75 | 30k    | 26m         |        |            |              |
|         | 10 |      |      |   0.69 | 30k    | 38m         |        |            |              |
|         | 15 |      |      |        | 30k    | (est 1h20m) |        |            |              |
|         | 20 |      |      |        | 30k    | (est 2h)    |        |            |              |
|         | 25 |      |      |        |        |             |        |            |              |
|---------+----+------+------+--------+--------+-------------+--------+------------+--------------|
| deep-EQ |  5 |      | 0.89 |        | 30k    | 43m         |        |            |              |
|         | 10 |      | 0.81 |        | 30k    | 1h46m       |        |            |              |
|         | 15 |      | 0.76 |        | 30k    | 3h40m       |        |            |              |
|         | 20 |      | 0.76 |        | 30k    | 6h51m       |        |            |              |
|         | 25 |      |      |        |        |             |        |            |              |
|         | 30 |      |      |        |        |             |        |            |              |


* Supervised full DAG TODO
** DONE universal model
CLOSED: [2020-05-11 Mon 09:52]
- [X] test on d=10-40
- [X] train on 13, 17, etc
- [ ] train on 25, 30, 35, 40, in smaller amount of steps


** STARTED Real data
- pairwise?
- non-pairwise but sparse?
- different graph type in synthetic data

** TODO Compare

*** TODO pairwise supervised model
Featurization:
- kernel mean embedding
- bivariate histogram

I'm probably just use my covariate matrix featurization.

Model:
- generalized linear model with l1 regularization
- 5 layer MLP

I'm probably just use a MLP

*** TODO traditional unsupervised model
- NOTEARS
- PC

- [ ] show concrete inference time
- [ ] generate simulated data from my julia code and try traditional algorithms

** TODO real evaluation: X -> sigma
Instead of starting from sigma, the real evaluation should:
1. use X, and compute sigma
2. ensure acyclic?


* Do-loss TODO

** New round
*** CANCELED use mixture multi-variant Gaussian to fit the data
CLOSED: [2020-01-09 Thu 18:28]
I should not use gaussian mixture, as that's not flexible, I need to know how
many components.

- observational
- 1 interventional
- 2+ interventional
- observational + 1 interventional
- observational + 2+ interventional

*** TODO use mixture Gaussian likelihood as oracle
to fit the do-loss

- I probably try to derive the closed form first

*** TODO sample interventions?
*** TODO sample from cyclic intermediate graph state


** TODO Implement interventional loss
*** TODO use dense GAN for graphical model
- generator
- discriminator

*** TODO implement causal effect inference (compute effect)
- [X] hard intervention
- soft intervention
- multiple interventions

*** TODO implement interventional loss
- random intervention
- compute effect
- discriminator likelyhood

*** Tuebingen pairs
- NOTEARS does not work
- implement LiNGRAM to see if it works
- But it does not seem to have interventional data, and does not have ground
  truth SEM to generate interventional data
- what are the interventional data out there?
- what is the optimization the meta-transfer is using?

*** related work
Looks like I have to compare against them, so it does not hurt implementing them now.

**** DONE The NOTEARS framework
CLOSED: [2019-12-11 Wed 17:00]
NOTEARS does not seem to recover beyond equivalent class.

I'm at the optimization solver, and currently
- Optim uses autodiff. However, it is slow, and does not seem to solve correctly
- I'm trying something else, like NLopt suite, and this seems to work

https://github.com/xunzheng/notears

***** DONE score metrics
CLOSED: [2019-12-11 Wed 18:09]
And the score seems to be very different when I modify notears's python code with:
- replace 2 * d * d with just the result and gradient of d*d
- the L1 regularizer also matters

***** DONE non-negative box constraint
CLOSED: [2019-12-11 Wed 18:09]
https://github.com/xunzheng/notears/issues/5

and related: 2*d*d w_est problem
***** TODO why NOTEARS can distinguish A->B and B->A?

**** DONE meta-transfer in julia
CLOSED: [2019-12-20 Fri 12:56]
And the +NOTEARS version

Several problems:
1. Zygote cannot differentiate through likelihood:
   https://github.com/FluxML/Zygote.jl/issues/436
2. Tracker.jl TrackedArrays cannot work through logsumexp's mapreduce

Thus it is basically impossible to get it work. But it's good enough, I
understand how it works, let's implement do-loss.

**** traditional
- PC
- LiNGRAM
- GES (using FGS)


*** TODO train with interventional loss

** More Ideas on Interventional loss
*** different interventions
- hard do-notation
- hard do-distribution
- soft intervention
- mechanism change

*** Separating interventional distributions
When there are many variables, and many interventions, and the interventional
data might be much less of amount then observational data, it might be
challenging to learn a generative model of the mixture distribution. Thus, we
might consider an extension of this work to seperated unknown interventions.

- we can learn generative models for each of the distribution
- we then use the minimum loss of discriminator to calculate interventional
  loss, i.e. as long as one of the interventional distribution is consistent
  with the random intervention, we accept it.


* old TODO-list

** TODO run those VAEs
** TODO run those GANs
** DONE julia?
   CLOSED: [2019-10-03 Thu 12:14]
Read flux.jl code
** TODO math equation data exp
** TODO interventional loss function exp

** clean up generative models
*** GAN
*** VAE

** unsupervised representation learning
*** InfoGAN
*** NOTEARS
*** Interventional Loss

** Causal generative models
*** TODO GAN
*** TODO VAE
*** TODO InfoGAN
*** TODO NOTEARS
*** TODO Causal

** Number of unique DAG

https://oeis.org/A003024

|  d | #dag |
|----+------|
|  1 |      |
|  2 |      |
|  3 |      |
|  4 |      |
|  5 |      |
|  6 |      |
|  7 |      |
|  8 |      |
|  9 |      |
| 10 |      |
| 11 |      |


** TODO run causal discovery

*** constraint based methods
- PC
- FCI: can handle confounders
*** score based
- Greedy Equivalence Search (GES)
- FGS

Scores:
- BIC
- AIC

*** inside equivalent class
non-Gaussian or non-Linear

- LiNGAM: Linear Non-Gaussian Acyclic Model:
  https://sites.google.com/site/sshimizu06/lingam
- no-linear model: seems to be extension to LiNGAM, do not have a special
  algorithm, still use noise footprint.


* DONE-list
** DONE Implement data generation
CLOSED: [2019-12-10 Tue 16:10]
- [X] random graph
  - Erdős-Rényi (ER)
  - scale-free (SF)
- [X] random weights
- gaussian noise
- [-] different models
  - [X] linear model
  - [ ] generalized linear
  - [ ] non-linear model
  - additive gaussian noise

** CANCELED Implement some cdt algorithms in Julia
CLOSED: [2019-12-06 Fri 18:08]
- CGNN
- NOTEARS
- DAG-GNN
- GraN-DAG
- meta-transfer

** CANCELED Implement traditional algorithms
CLOSED: [2019-12-06 Fri 18:08]
- [X] PC
- [X] FCI
- LiNGRAM
- GIES
- CAM

** DONE GAN for MNIST
CLOSED: [2019-12-18 Wed 12:42]
*** DONE test python code for GAN
CLOSED: [2019-12-10 Tue 16:07]
*** DONE debug DCGAN for julia
CLOSED: [2019-12-10 Tue 16:06]

My experience:
- dropout is very important, use in discriminator
- dropout can be used together with batchnorm
- use bias = false is not required
- the last conv in generator should not be stride=2, but 1, i.e. no scaling
  performed in the last conv
- normalize MNIST to -1,1 instead of 0,1 seems to be very important

*** Python GAN references
- PyTorch-GAN 5k stars https://github.com/eriklindernoren/PyTorch-GAN
- https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
- tensorflow official DCGAN
- pytorch official DCGAN

** CANCELED supervised re-parametric
CLOSED: [2019-12-18 Wed 12:41]
Train:
- assume linear gaussian model
- design a new convolution (on graph), probably use GCN
- generate random model
- generate mu and sigma
- train f(mu, sigma)=adjacent_matrix

Open problems:
- conv operator, shared weights
- order of nodes
- generalized reparametric

Inference:
- compute statistics mu and sigma
- f(mu, sigma)
*** TODO ensure acyclic in evaluation

*** DONE test on different graph
CLOSED: [2019-12-16 Mon 14:31]
for 5-var case, to see what's the scalability problem

It does not work, so the problem is not the size of graph. With different graphs
as test data, it does not work.

But there is some good news:
1. NN fits training data without any problem
2. with more N per graph, it improves

*** TODO reuse 5-var learned model on 20 var

*** TODO Regularizing
- it can reach high accuracy very quickly
- but it seems to overfitting very quickly as well

*** CANCELED negative sampling
CLOSED: [2019-12-12 Thu 16:34]
Use a distance measure, or AIC/BIC score to assign the score for all (or a
sample) graphs, not just the correct graph

*** TODO multi-variate
*** TODO stochastic SGD
- I need more data
- when data is large, I need to batch
- and I need to shuffle the batches as well, for stochastic
*** TODO other models
- different number of hidden units
- different number of layers
- different models, e.g. GCN

*** TODO sharing weights
For generalizing to other number of variables
*** TODO generalizing to other models
- linear + Gaussian
- non-Gaussian, e.g. poisson
- generalized linear
- non-linear


*** Intergrate with (noise-based?) assymetrcis
- For distinguishing equivalent classes
  - but it alrady works very well
- using data other than statistics


* Supervised full DAG DONE

** China
*** DONE Look for other potential bugs in equivariant model
CLOSED: [2020-03-01 日 14:03]
Mostly the correcteness of gradient calculation and broadcasting.

*** DONE use cross entropy
CLOSED: [2020-02-29 六 17:36]
*** DONE figure out how to make it run faster
CLOSED: [2020-02-29 六 17:36]
*** CANCELED precision is low, but recall is high
CLOSED: [2020-03-01 日 14:00]
** CANCELED supervised learning with equivariant model
CLOSED: [2020-01-12 Sun 12:40]

Does not work.

*** large n
*** verify gradient computation
*** add bias
*** use max-pool
*** normalization & regularization

*** DONE gpu
CLOSED: [2020-01-12 Sun 11:55]
*** DONE more data points
CLOSED: [2020-01-12 Sun 11:55]
Does not help.

*** DONE init function
CLOSED: [2020-01-12 Sun 12:40]
simply 0

This does not work, the model parameters does not change at all.

*** DONE compare parameters
CLOSED: [2020-01-12 Sun 12:39]

*** DONE try previous MLP
CLOSED: [2020-01-10 Fri 13:01]

Delta debuggging what is wrong

It is the 1 dim (5,5,1,100)


** DONE exp model setting
CLOSED: [2020-03-02 一 17:34]

- dropout, batchnorm or None
  - looks like dropout is not working
- nlayer
- depth
- width
- activation
- learning rate

** DONE performance of FC models
CLOSED: [2020-03-02 一 17:34]
Why it is slow bad now? Probably:
- data generating with [0.5,2]
- threshold
- sigmoid activation
- MSE loss


** DONE data
CLOSED: [2020-03-02 一 17:34]

Data:
- unit data (W=1)
- [-2,-0.5] data
- non-univariance data
- non-linear model

** DONE loss
CLOSED: [2020-03-02 一 17:34]
Loss:
- cross entropy vs. MSE

use MSE, because they seems to perform similarly, and xent only supports binary
classification.

** CANCELED learning rate (decay)

** TODO persistence
*** DONE integrate with reading tensorboard logs
CLOSED: [2020-03-04 三 21:35]

*** DONE saved model
CLOSED: [2020-03-04 三 21:34]
- save at multiple points: not very urgent

For synthetic data, not very useful, because tensorboard already logs the loss
and accuracy metrics.

For real data this is necessary.

*** CANCELED continue training
CLOSED: [2020-03-02 一 19:36]
with unique ID support

This might not make sense, because
1. I need to implement logic of resuming
2. I need to implement logic for restoring "step" count
3. the time metrics would be wrong

A lot of overhead.

*** CANCELED seed and averaging
CLOSED: [2020-03-02 一 19:36]

Not very urgent.
*** CANCELED profiling
CLOSED: [2020-03-02 一 17:50]
If profiling is not costly, I can probably record for each run

** DONE verify performance
CLOSED: [2020-03-06 五 12:24]
Wait for all runs and see:
- debug dropout performance
- debug FC vs. EQ and -deep version performance

** DONE universal EQ model
CLOSED: [2020-03-12 四 14:37]

Note that EQ model parameters does not depend on d. Each layer has exactly 5
weights, no matter the size of graph.

Thus it may be possible to train a universal EQ model that works on different
graph. I can approach this in several ways:
1. [X] directly transfer
   1. fine tune some layer?
2. [X] train on different size graphs

Looks like the direct transfer works.

*** DONE train on different size graphs
CLOSED: [2020-03-12 四 14:37]

- [X] remove d in model, make it general

** DONE continual training
CLOSED: [2020-03-12 四 12:36]

1. choose a larger save steps, e.g. 1k
2. save the model as modelID-1000.bson
3. when continual training, check if model loadable. Choose the most recent, and
   set current steps accordingly

NOTE: the tensorboard logs must be kept consistent, and only do append

The only downside would be the time will be inaccurate. Maybe I should (HEBI:
record time as well in filename) (HEBI: record time as well in filename ...),
together with steps. After all, these are the only two thing I need.



*** DONE continual training debug
CLOSED: [2020-03-12 四 12:36]
*** DONE cudatasetiterator convert debug
CLOSED: [2020-03-12 四 11:04]


** DONE debug EQ performance
CLOSED: [2020-03-12 四 14:37]
Not matching previous. Maybe previous result is binary C=1? Try it, and write
exp option for this.

Actually the deep model seems to preserve the performance.
** CANCELED Synthetic data
CLOSED: [2020-03-12 四 23:29]
- table for different models and settings
  - network settings:
    - dropout batchnorm vs. none
    - network depth
    - network width
    - FC vs. EQ
    - batch size
    - learning rate
  - metrics
    - accuracy and recall
    - # iterations
    - time
    - # parameters
- plot AUC for different threshold
- plot training process

** DONE generate data beforehand, various of configs
CLOSED: [2020-05-17 Sun 20:53]
- [X] graph size d
- [X] weight matrix range
  - weights range [0.5, 1.5*k] for k=1/3, 1/2, 1, 2, 3, ..., ?
- [X] graph type: ER, SF
- [X] noise model: Gaussian, TODO


*** DONE the result parsing code
CLOSED: [2020-05-19 Tue 17:25] SCHEDULED: <2020-05-19 Tue>
*** DONE restore previous results
CLOSED: [2020-05-19 Tue 17:25] SCHEDULED: <2020-05-19 Tue>
Probably test previous correlation matrix approach

- [X] why FC models are so bad now? previously it is also bad

*** DONE the experiment code
CLOSED: [2020-05-17 Sun 23:04]
*** CANCELED performance and GPU utilization
CLOSED: [2020-05-18 Mon 23:10]

*** DONE add CNN as baseline
CLOSED: [2020-05-20 Wed 12:13] SCHEDULED: <2020-05-19 Tue>

Probably two kinds of CNNs:
- CNN with auto-encoder style bottleneck
- CNN without any bottleneck, with same padding in all layers

*** DONE transfer or not-transfer
CLOSED: [2020-05-20 Wed 12:46]
- I should do one set of experiment for non-transfer, i.e. one model for a grpah
  size d.
- I should later do another experiment, specifically testing transferability and
  ensemble training. This should include both CNN and Eq models

*** DONE more variations
CLOSED: [2020-05-21 Thu 20:18]
- [X] ER, SF, and others?
  - [X] ER-1 ER-2 ER-4: with d, 2d, 4d edges
  - [X] SF-1 SF-2 SF-4
  - [-] (CANCELED) graph generation: triangular binary matrix, each generated as
    Bern(0.5), then permutate the variables
- [-] Gaussian, Poisson, and others?
  - [X] Exponential
  - [X] Gumbel
  - [-] what is used in LiNGRAM cite:cite:2006-JMLR-Shimizu-Linear Seems to be
    1. generate Gaussian noise e
    2. use e^a where a is uniform from the interval [0.5, 0.8] or [1.2, 2.0]
- [-] non-linear mechanism?
  - [-] linear and quadratic terms
  - [-] Gaussian process with RBF kernel
  - [X] MLP: one hidden layer of size 10 and sigmoid activation
  - Sobolev basis

Gaussian prcess

#+BEGIN_SRC python
from sklearn.gaussian_process import GaussianProcessRegressor
gp = GaussianProcessRegressor()
X = np.random.randn(10, 3)
gp.sample_y(np.concatenate((X,X)), random_state=None).flatten()
gp.sample_y(np.concatenate((X,X)), random_state=None).flatten()
#+END_SRC

However, there's no random function (=sample_y=) equivalent in Julia
GaussianProcesses libraries, thus I'm not using it.

*** CANCELED use json for on julia-side result collection as well
CLOSED: [2020-05-31 Sun 01:12]
