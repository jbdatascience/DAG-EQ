#+TITLE: Implementation notes

* References
** Code from papers
- https://github.com/GoudetOlivier/CGNN, good quality, heavy in OO, also a copy in CDT
- CausalDiscoveryToolbox
  https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox has an
  implementation of NCC.
- https://github.com/Diviyan-Kalainathan/SAM
- NOTEARS https://github.com/xunzheng/notears
- DAG-GNN https://github.com/fishmoon1234/DAG-GNN, good quality
- GraN-DAG https://github.com/kurowasan/GraN-DAG
- meta-transfer: https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon

* Supervised full DAG TODO

** TODO performance of FC models
Why it is slow bad now? Probably:
- data generating with [0.5,2]
- threshold
- sigmoid activation
- MSE loss

** data

Data:
- unit data (W=1)
- [-2,-0.5] data
- non-univariance data
- non-linear model

** Real data
- pairwise?
- non-pairwise but sparse?

** loss
Loss:
- cross entropy vs. MSE

** setting
Network setting:
- dropout batchnorm vs. none
- network depth
- network width
- simple FC
  - running time
  - convergence
  - # of parameters

Other settings:
- threshold
- batch size
- learning rate (decay)

** Compare
- pairwise supervised model
- unsupervised model
  - NOTEARS
  - PC

** TODO persistence

*** profiling
If profiling is not costly, I can probably record for each run
*** saved model
- save at multiple points
*** continue training
with unique ID support

*** seed and averaging

** TODO Results and Plotting

* Do-loss TODO

** New round
*** CANCELED use mixture multi-variant Gaussian to fit the data
CLOSED: [2020-01-09 Thu 18:28]
I should not use gaussian mixture, as that's not flexible, I need to know how
many components.

- observational
- 1 interventional
- 2+ interventional
- observational + 1 interventional
- observational + 2+ interventional

*** TODO use mixture Gaussian likelihood as oracle
to fit the do-loss

- I probably try to derive the closed form first

*** TODO sample interventions?
*** TODO sample from cyclic intermediate graph state


** TODO Implement interventional loss
*** TODO use dense GAN for graphical model
- generator
- discriminator

*** TODO implement causal effect inference (compute effect)
- [X] hard intervention
- soft intervention
- multiple interventions

*** TODO implement interventional loss
- random intervention
- compute effect
- discriminator likelyhood

*** Tuebingen pairs
- NOTEARS does not work
- implement LiNGRAM to see if it works
- But it does not seem to have interventional data, and does not have ground
  truth SEM to generate interventional data
- what are the interventional data out there?
- what is the optimization the meta-transfer is using?

*** related work
Looks like I have to compare against them, so it does not hurt implementing them now.

**** DONE The NOTEARS framework
CLOSED: [2019-12-11 Wed 17:00]
NOTEARS does not seem to recover beyond equivalent class.

I'm at the optimization solver, and currently
- Optim uses autodiff. However, it is slow, and does not seem to solve correctly
- I'm trying something else, like NLopt suite, and this seems to work

https://github.com/xunzheng/notears

***** DONE score metrics
CLOSED: [2019-12-11 Wed 18:09]
And the score seems to be very different when I modify notears's python code with:
- replace 2 * d * d with just the result and gradient of d*d
- the L1 regularizer also matters

***** DONE non-negative box constraint
CLOSED: [2019-12-11 Wed 18:09]
https://github.com/xunzheng/notears/issues/5

and related: 2*d*d w_est problem
***** TODO why NOTEARS can distinguish A->B and B->A?

**** DONE meta-transfer in julia
CLOSED: [2019-12-20 Fri 12:56]
And the +NOTEARS version

Several problems:
1. Zygote cannot differentiate through likelihood:
   https://github.com/FluxML/Zygote.jl/issues/436
2. Tracker.jl TrackedArrays cannot work through logsumexp's mapreduce

Thus it is basically impossible to get it work. But it's good enough, I
understand how it works, let's implement do-loss.

**** traditional
- PC
- LiNGRAM
- GES (using FGS)


*** TODO train with interventional loss

** More Ideas on Interventional loss
*** different interventions
- hard do-notation
- hard do-distribution
- soft intervention
- mechanism change

*** Separating interventional distributions
When there are many variables, and many interventions, and the interventional
data might be much less of amount then observational data, it might be
challenging to learn a generative model of the mixture distribution. Thus, we
might consider an extension of this work to seperated unknown interventions.

- we can learn generative models for each of the distribution
- we then use the minimum loss of discriminator to calculate interventional
  loss, i.e. as long as one of the interventional distribution is consistent
  with the random intervention, we accept it.


* old TODO-list

** TODO run those VAEs
** TODO run those GANs
** DONE julia?
   CLOSED: [2019-10-03 Thu 12:14]
Read flux.jl code
** TODO math equation data exp
** TODO interventional loss function exp

** clean up generative models
*** GAN
*** VAE

** unsupervised representation learning
*** InfoGAN
*** NOTEARS
*** Interventional Loss

** Causal generative models
*** TODO GAN
*** TODO VAE
*** TODO InfoGAN
*** TODO NOTEARS
*** TODO Causal

** Number of unique DAG

https://oeis.org/A003024

|  d | #dag |
|----+------|
|  1 |      |
|  2 |      |
|  3 |      |
|  4 |      |
|  5 |      |
|  6 |      |
|  7 |      |
|  8 |      |
|  9 |      |
| 10 |      |
| 11 |      |


** TODO run causal discovery

*** constraint based methods
- PC
- FCI: can handle confounders
*** score based
- Greedy Equivalence Search (GES)
- FGS

Scores:
- BIC
- AIC

*** inside equivalent class
non-Gaussian or non-Linear

- LiNGAM: Linear Non-Gaussian Acyclic Model:
  https://sites.google.com/site/sshimizu06/lingam
- no-linear model: seems to be extension to LiNGAM, do not have a special
  algorithm, still use noise footprint.


* DONE-list
** DONE Implement data generation
CLOSED: [2019-12-10 Tue 16:10]
- [X] random graph
  - Erdős-Rényi (ER)
  - scale-free (SF)
- [X] random weights
- gaussian noise
- [-] different models
  - [X] linear model
  - [ ] generalized linear
  - [ ] non-linear model
  - additive gaussian noise

** CANCELED Implement some cdt algorithms in Julia
CLOSED: [2019-12-06 Fri 18:08]
- CGNN
- NOTEARS
- DAG-GNN
- GraN-DAG
- meta-transfer

** CANCELED Implement traditional algorithms
CLOSED: [2019-12-06 Fri 18:08]
- [X] PC
- [X] FCI
- LiNGRAM
- GIES
- CAM

** DONE GAN for MNIST
CLOSED: [2019-12-18 Wed 12:42]
*** DONE test python code for GAN
CLOSED: [2019-12-10 Tue 16:07]
*** DONE debug DCGAN for julia
CLOSED: [2019-12-10 Tue 16:06]

My experience:
- dropout is very important, use in discriminator
- dropout can be used together with batchnorm
- use bias = false is not required
- the last conv in generator should not be stride=2, but 1, i.e. no scaling
  performed in the last conv
- normalize MNIST to -1,1 instead of 0,1 seems to be very important

*** Python GAN references
- PyTorch-GAN 5k stars https://github.com/eriklindernoren/PyTorch-GAN
- https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
- tensorflow official DCGAN
- pytorch official DCGAN

** CANCELED supervised re-parametric
CLOSED: [2019-12-18 Wed 12:41]
Train:
- assume linear gaussian model
- design a new convolution (on graph), probably use GCN
- generate random model
- generate mu and sigma
- train f(mu, sigma)=adjacent_matrix

Open problems:
- conv operator, shared weights
- order of nodes
- generalized reparametric

Inference:
- compute statistics mu and sigma
- f(mu, sigma)
*** TODO ensure acyclic in evaluation

*** DONE test on different graph
CLOSED: [2019-12-16 Mon 14:31]
for 5-var case, to see what's the scalability problem

It does not work, so the problem is not the size of graph. With different graphs
as test data, it does not work.

But there is some good news:
1. NN fits training data without any problem
2. with more N per graph, it improves

*** TODO reuse 5-var learned model on 20 var

*** TODO Regularizing
- it can reach high accuracy very quickly
- but it seems to overfitting very quickly as well

*** CANCELED negative sampling
CLOSED: [2019-12-12 Thu 16:34]
Use a distance measure, or AIC/BIC score to assign the score for all (or a
sample) graphs, not just the correct graph

*** TODO multi-variate
*** TODO stochastic SGD
- I need more data
- when data is large, I need to batch
- and I need to shuffle the batches as well, for stochastic
*** TODO other models
- different number of hidden units
- different number of layers
- different models, e.g. GCN

*** TODO sharing weights
For generalizing to other number of variables
*** TODO generalizing to other models
- linear + Gaussian
- non-Gaussian, e.g. poisson
- generalized linear
- non-linear


*** Intergrate with (noise-based?) assymetrcis
- For distinguishing equivalent classes
  - but it alrady works very well
- using data other than statistics


* Supervised full DAG DONE

** China
*** DONE Look for other potential bugs in equivariant model
CLOSED: [2020-03-01 日 14:03]
Mostly the correcteness of gradient calculation and broadcasting.

*** DONE use cross entropy
CLOSED: [2020-02-29 六 17:36]
*** DONE figure out how to make it run faster
CLOSED: [2020-02-29 六 17:36]
*** CANCELED precision is low, but recall is high
CLOSED: [2020-03-01 日 14:00]
** CANCELED supervised learning with equivariant model
CLOSED: [2020-01-12 Sun 12:40]

Does not work.

*** large n
*** verify gradient computation
*** add bias
*** use max-pool
*** normalization & regularization

*** DONE gpu
CLOSED: [2020-01-12 Sun 11:55]
*** DONE more data points
CLOSED: [2020-01-12 Sun 11:55]
Does not help.

*** DONE init function
CLOSED: [2020-01-12 Sun 12:40]
simply 0

This does not work, the model parameters does not change at all.

*** DONE compare parameters
CLOSED: [2020-01-12 Sun 12:39]

*** DONE try previous MLP
CLOSED: [2020-01-10 Fri 13:01]

Delta debuggging what is wrong

It is the 1 dim (5,5,1,100)

