#+TITLE: Causal Variational Auto Encoder

* Experimental Design

I'm thinking of using a equation system. The equation is of form =x1 +
x2 * x3 -> y=. Changing x will change y, while change y will not
change X. The dataset will have mixed observational and interventional
data.


* Meeting notes

1. I want to talk about an application that is relatively simple and
   probably doable. Then some technical details.

2. How to design an application? It should be real, high dimensional
   (image), there needs to be causality, and we need to know ground
   truth. Traditional causality application such as 
   - (cancer, smoking), (gene regulator network), (student
     intelligence, GPA) don't have high dimensional application
     mapping. 
   - (weather, floor wet) kinds of application requires heavy image
     recognition capabilities which is still unsolved problem even in
     supervised learning.
   - causality ground truth is missing, as no causality dataset
     available. Creating that dataset would require huge amount of
     effort.


Several key features
- real
- high dimensional (image)
- causality ground truth
- suitable for linear models
- simple and potential doable

Thus, we propose a simple but powerful application experiment. We use
*one-directional math equations, presented in images*. This brings
causality, latent structure, as well as high dimensional
data. Consider the following equations:

#+BEGIN_EXAMPLE
a + b -> c

4 + 3     -> 7
4 + 5(3)  -> [9]
2 + 8     -> 10
1 + 3     -> 4
1 + 3     -> 7(4)
#+END_EXAMPLE

Application input/output: input data is a set of such math equations
in images. The desired functionality: if using changes an equation by
modifying some digits in the image, the equation should be updated
accordingly.

Why it is real: there are three latent variables, a,b,c. The causal
model is ~c = a+b~. Using this model, we can generate data for
a,b,c. We can use MNIST dataset to generate image for them. These
images will be the dataset, with known causality ground truth model,
and it's real data because the data is just hand written digits.

The ground truth causal model: The presence of direction in the model
is the key to causality. It means if a or b changes, c would
change. But if c changes, a and b would not change. This is also a
well-known example of causality, and well suited in linear model.

Thus when considering the causality, the task is, when the image is
modified at place a or b, we can generate an output image with updated
c. If the image is modified at place of c, the output image should not
change a and b.

Un-directional counterpart (Or generative models for equations): if we
don't consider the directions, it is still a graphical model,
basically a Markov model. It can be thought in terms of conditional
generative model: condition on one latent variable to be fixed to the
change, and generate an image with valid equations. In this way, we
don't need to specify the model has a structure, and let the
generative model (GAN or VAE) figures it out automatically. But it may
be very hard to train the model successfully in such an unsupervised
setting, the search space is too big. Introducing a Markov network in
the latent layer may help. But this is still fundamentally different
than causal model. The generative approach simply cannot work for
causal model, it does not have directions.

There are still several difficulties about this application:
1. May be hard to learn image to digit mapping. Since it is
   unsupervised, we are not giving the labels. Thus it might be hard
   for the model to engineering the recognition of hand-written
   digits.
2. Multiple digits: we will have multiple digits in the image. Image
   object detection is not a solved problem. It should be even harder
   for unsupervised learning. There's only recent effort (2017 Hitton
   CapsuleNetwork) to deal with overlapping digits in MNIST dataset.
3. We will need more latent code:
4. Mixed observational and interventional data can be helpful, it
   might be orthogonal to the problem of learning from high
   dimentional data. Thus, interventional loss function may be used in
   traditional low-dimentional causal discovery.
5. Detailed design of interventional loss function:
   - Assume there will be one variable changing at a time
   - We train a GAN for data $X$. The discriminator of the GAN model
     will accept an $x$, and output [0,1] with 1 for true sample and 0
     for fake. We will use the discriminator in the loss function when
     we train VAE model
   - VAE model has an encoder from x to z, and an decoder from z to
     xout. There is a prior on z. The loss function was
     $CrossEntropy(x,xout) + KL(prior(x), posterior(z|x))$. We use a
     Gaussian distribution for prior of z, and a structured posterior
     with causal weight parameters.
   - Now we add another term. For each x, once we get z, we compute
     $z'^{(i)}=p(z|z_i=a)$ and $z''^{(i)} = p(z|do(z_i=a))$. Note that
     this is not a distribution, but a concrete data point for each
     data point x. Then, we decode z' and z'' to obtain x' and
     x''. The third loss function term, the interventional term, is
     written as $-\sum_{i} (D(x'') - D(x'))$. The $D$ here is the
     discriminator of the pretrained GAN, but it can be any other
     discriminator. We need such a "distribution-level" discriminator
     because, $x''$ may not be compared with this specific data point
     $x$. The discriminator of GAN is probably the most general
     one. We have the summation to try an intervention for each latent
     variable in z, but it can also be a random $z_i$, no big deal.
   - Training feasibility: the discriminator is typically a stack of
     CNN layers. Thus gradients can be back-propagated.
6. We need more latent features than the variables in the equation,
   for an obvious reason: assume each latent variable means the value
   of a digit (so that we can fit the causal model). Then we still
   need to consider the look of the digit. Thus, there needs to be
   a random noise vector associated with each causal variable.







* Related VAE Implementations
- https://github.com/ericjang/draw
- https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW
- https://github.com/openai/iaf
- https://github.com/hwalsuklee/tensorflow-mnist-VAE
- https://github.com/oduerr/dl_tutorial/tree/master/tensorflow/vae
- https://github.com/stitchfix/fauxtograph
- DCGAN: https://github.com/carpedm20/DCGAN-tensorflow

edward: a package with variational inference, not auto encoder
https://github.com/blei-lab/edward

Jonathan Hui
- a really good post:
  https://jhui.github.io/2017/03/06/Variational-autoencoders/
- Code:
  https://github.com/jhui/deep_learning/tree/master/variational_autoencoder


keras:
- blog (very good):
  https://blog.keras.io/building-autoencoders-in-keras.html
- code (not necessarily company to above post):
  https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py
- https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder_deconv.py
- with concrete latent distribution (by someone else)
  https://github.com/EmilienDupont/vae-concrete
- keras tutorial: Keras as a simplified interface to TensorFlow
  https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html

y0ast:
- https://github.com/y0ast/Variational-Autoencoder
- https://github.com/y0ast/VAE-Torch
- https://github.com/y0ast/VAE-TensorFlow

Hyperspherical Variational Auto-Encoders
- blog: https://nicola-decao.github.io/s-vae
- https://github.com/nicola-decao/s-vae-tf

kvfrans:
- blog: http://kvfrans.com/variational-autoencoders-explained/
  - the code: https://github.com/kvfrans/variational-autoencoder

A good blog post:
- https://jmetzen.github.io/2015-11-27/vae.html

Blog post
- Part 1:
  https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html
- Part 2 (TODO):
  https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html
- Code: https://github.com/fastforwardlabs/vae-tf/

Blog (TODO):
- https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
- code: https://github.com/altosaar/variational-autoencoder

Jeremy Jordan
- https://www.jeremyjordan.me/autoencoders/
- https://www.jeremyjordan.me/variational-autoencoders/

A pdf tutorial on Arxiv:
- https://arxiv.org/abs/1606.05908

Other posts:
- no code https://ermongroup.github.io/cs228-notes/extras/vae/
- https://www.doc.ic.ac.uk/~js4416/163/website/neural-networks/index.html

